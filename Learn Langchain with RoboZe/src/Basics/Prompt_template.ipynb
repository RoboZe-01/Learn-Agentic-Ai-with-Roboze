{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84602748",
   "metadata": {},
   "source": [
    "## Prompt template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361939b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3561608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the api and the model \n",
    "\n",
    "import os \n",
    "from dotenv import load_dotenv , find_dotenv\n",
    "_=load_dotenv(find_dotenv())\n",
    "gemini_api_key = os.environ[\"GOOGLE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2683b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading completion model \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llmModel = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\",temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ab2e7",
   "metadata": {},
   "source": [
    "### Prompts and prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is for completion model \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a template\n",
    "prompt_template  = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} story on {topic} in 20 words\"\n",
    ")\n",
    "\n",
    "llmModelPrompt = prompt_template.format(\n",
    "    adjective = \"Informative\",\n",
    "    topic=\"langchain\"\n",
    ")\n",
    "\n",
    "result = llmModel.invoke(llmModelPrompt)\n",
    "# print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0445d",
   "metadata": {},
   "source": [
    "## chat completion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c620db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "chatmodel = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff58d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course.\n",
      "\n",
      "The full form of **RAG** is:\n",
      "\n",
      "**Retrieval-Augmented Generation**\n",
      "\n",
      "---\n",
      "\n",
      "As an AI engineer, let me break that down for you, because the name perfectly describes what it does:\n",
      "\n",
      "### 1. Retrieval\n",
      "This is the \"search\" or \"look-up\" phase. Before the AI generates an answer, it first **retrieves** relevant and up-to-date information from a specific knowledge source. This source could be:\n",
      "*   A company's internal documents\n",
      "*   A specific set of PDFs\n",
      "*   A database\n",
      "*   A website's content\n",
      "\n",
      "### 2. Augmented\n",
      "The information that was retrieved is then used to **augment** (or enhance) the prompt that is sent to the Large Language Model (LLM). In simple terms, the system tells the AI:\n",
      "\n",
      "> *\"Here is the user's question: '[User's Original Question]'. Now, use the following information to help you form your answer: '[The Retrieved Documents]'.\"*\n",
      "\n",
      "### 3. Generation\n",
      "This is the final step where the LLM, now equipped with both the user's original question and the specific, relevant context, **generates** a response.\n",
      "\n",
      "### Why is RAG so important?\n",
      "\n",
      "It solves two of the biggest problems with standard LLMs:\n",
      "\n",
      "1.  **Hallucinations (Making things up):** By forcing the model to base its answer on the provided documents, RAG significantly reduces the chances of the AI inventing incorrect facts. The answer is \"grounded\" in a source of truth.\n",
      "2.  **Stale Knowledge:** A model like GPT-4 has a knowledge cut-off date. It doesn't know about events that happened after its training. RAG connects the model to a live, up-to-date knowledge base, allowing it to answer questions about recent information.\n",
      "\n",
      "**Analogy:** Think of it as an \"open-book exam.\" A standard LLM is like a student answering from memory (a closed-book exam). A RAG system is like a student who can look up the correct facts in the textbook before writing their answer. The result is far more accurate and reliable.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chatTemplate = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You're an {profession} expert on {topic}.\"),\n",
    "        (\"human\",\"Hello, Mr {profession},can you please answer a question?\"),\n",
    "        (\"ai\",\"sure\"),\n",
    "        (\"human\",\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "messages = chatTemplate.format_messages(\n",
    "    profession = \"Ai engineer\",\n",
    "    topic=\"RAG\",\n",
    "    user_input=\"full form of rag\"\n",
    ")\n",
    "response=chatmodel.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bf861",
   "metadata": {},
   "source": [
    "## Few Shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd45198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\"input\":\"hii\",\"output\":\"hola\"},\n",
    "    {\"input\":\"bye\",\"output\":\"adios\"},\n",
    "]\n",
    "example_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\",\"input\"),\n",
    "        (\"ai\",\"output\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "       (\"system\",\"you are an English-spanish translator.\"\n",
    "        ),few_shot_prompt,\n",
    "        (\"human\",\"{input}\")\n",
    "    \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd0aa0",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a0150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a few ways to say this, depending on the level of formality:\n",
      "\n",
      "*   **¿Cómo estás?** (Informal, used with friends, family, people your age)\n",
      "*   **¿Cómo está?** (Formal, used with elders, strangers, or in professional settings)\n",
      "*   **¿Qué tal?** (A very common, more casual greeting, similar to \"How's it going?\" or \"What's up?\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Making the chain\n",
    "chain = final_prompt | chatmodel         # Langchain Expression language\n",
    "\n",
    "res=chain.invoke({\"input\":\"How are you?\"})\n",
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
